{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('loyal': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ba8018fc530ee8129fef2ec252aae2b62563187ee120605478eab305cf687709"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Notebook to perform intent classification on the data from  https://github.com/clinc/oos-eval\n",
    "### Use data_full.json file "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/nikunjkotecha/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "nltk.download( 'stopwords' )"
   ]
  },
  {
   "source": [
    "### Load the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                sent        intent\n",
       "0  does ireland have any travel alerts i should b...  travel_alert\n",
       "1  does north korea have any travel alerts i shou...  travel_alert\n",
       "2             are there any travel alerts for russia  travel_alert\n",
       "3  does spain have any travel alerts i should be ...  travel_alert\n",
       "4        are there any travel alerts for north korea  travel_alert"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sent</th>\n      <th>intent</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>does ireland have any travel alerts i should b...</td>\n      <td>travel_alert</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>does north korea have any travel alerts i shou...</td>\n      <td>travel_alert</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>are there any travel alerts for russia</td>\n      <td>travel_alert</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>does spain have any travel alerts i should be ...</td>\n      <td>travel_alert</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>are there any travel alerts for north korea</td>\n      <td>travel_alert</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "class TextData:\n",
    "    def __init__( self, file ):\n",
    "        self.file = file\n",
    "    \n",
    "    def read_json( self, set='train', cols=['sent'], target='intent' ):\n",
    "        '''\n",
    "        read the json file and obtained the in-scope set\n",
    "        '''\n",
    "        with open( self.file, 'r' ) as f:\n",
    "            data = json.load( f )\n",
    "        df = pd.DataFrame( data[set], columns=cols+[target] )\n",
    "        return df\n",
    "    \n",
    "    def random_labels( self, target='intent', n=20, seed=0 ):\n",
    "        '''\n",
    "        choose 'n' random intent classes\n",
    "        '''\n",
    "        df = self.read_json( target=target )\n",
    "        np.random.seed( seed )\n",
    "        labels = np.random.choice( df[target].unique(), size=n, replace=False )\n",
    "        labels = { val:idx for idx,val in enumerate( labels ) }\n",
    "        return labels\n",
    "    \n",
    "    def get_set( self, set, labels, target='intent' ):\n",
    "        '''\n",
    "        obtained the in-scope set of given intent classes\n",
    "        '''\n",
    "        df = self.read_json( set=set )\n",
    "        df = df[ df[target].isin( labels.keys() ) ].reset_index( drop=True )\n",
    "        return df\n",
    "    \n",
    "file = 'data_full.json'\n",
    "data = TextData( file )\n",
    "# obtain random intent classes\n",
    "labels = data.random_labels()\n",
    "\n",
    "# get the training and validation set\n",
    "train = data.get_set( 'train', labels )\n",
    "val = data.get_set( 'val', labels )\n",
    "train.head()"
   ]
  },
  {
   "source": [
    "### Data Cleaning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                sent        intent  \\\n",
       "0  does ireland have any travel alerts i should b...  travel_alert   \n",
       "1  does north korea have any travel alerts i shou...  travel_alert   \n",
       "2             are there any travel alerts for russia  travel_alert   \n",
       "3  does spain have any travel alerts i should be ...  travel_alert   \n",
       "4        are there any travel alerts for north korea  travel_alert   \n",
       "\n",
       "                            clean  \n",
       "0      ireland travel alert aware  \n",
       "1  north korea travel alert aware  \n",
       "2             travel alert russia  \n",
       "3        spain travel alert aware  \n",
       "4        travel alert north korea  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sent</th>\n      <th>intent</th>\n      <th>clean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>does ireland have any travel alerts i should b...</td>\n      <td>travel_alert</td>\n      <td>ireland travel alert aware</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>does north korea have any travel alerts i shou...</td>\n      <td>travel_alert</td>\n      <td>north korea travel alert aware</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>are there any travel alerts for russia</td>\n      <td>travel_alert</td>\n      <td>travel alert russia</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>does spain have any travel alerts i should be ...</td>\n      <td>travel_alert</td>\n      <td>spain travel alert aware</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>are there any travel alerts for north korea</td>\n      <td>travel_alert</td>\n      <td>travel alert north korea</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "class TextCleaning:\n",
    "    def __init__( self ):\n",
    "        # punctuations, stop word, lemmatizer\n",
    "        self.punctuations = string.punctuation\n",
    "        self.stop_words = stopwords.words( 'english' )\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def process( self, sentence ):\n",
    "        '''\n",
    "        remove punctuations, stop word to a given sentence\n",
    "        then lemmatize each word\n",
    "        '''\n",
    "        temp = []\n",
    "        for w in word_tokenize( sentence ):\n",
    "            if w not in self.punctuations and w not in self.stop_words:\n",
    "                w = self.lemmatizer.lemmatize(w)\n",
    "                if w not in temp:\n",
    "                    temp.append(w)\n",
    "        return ' '.join( temp )\n",
    "\n",
    "    def preprocess( self, df, col='sent' ):\n",
    "        '''\n",
    "        accept a dataframe and preprocess its sentences\n",
    "        '''\n",
    "        # clean the data\n",
    "        df['clean'] = df.apply( lambda row: self.process( row[col] ), axis=1 )\n",
    "        return df\n",
    "\n",
    "data_cleaning = TextCleaning()\n",
    "# clean the training and validation set\n",
    "train = data_cleaning.preprocess( train )\n",
    "val = data_cleaning.preprocess( val )\n",
    "train.head()"
   ]
  },
  {
   "source": [
    "### Performing feature extraction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((2000, 1038), (2000,))"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "class FeatureExtraction:\n",
    "    '''\n",
    "    This data contains textual data. In order to train the model,\n",
    "    we need text embeddings. To obtain these embeddings, we can use\n",
    "    tfidf approach\n",
    "    '''\n",
    "    def __init__( self ):\n",
    "        pass\n",
    "\n",
    "    def tfidf_fit( self, df, col='clean' ):\n",
    "        '''\n",
    "        fit the tfidf to the training data\n",
    "        '''\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        vectorizer.fit( df[col] )\n",
    "        return vectorizer\n",
    "\n",
    "    def tfidf_extract( self, df, vectorizer, col='clean' ):\n",
    "        '''\n",
    "        extract tfidf features on the data\n",
    "        '''\n",
    "        X = vectorizer.transform( df[col] )\n",
    "        return X.toarray()\n",
    "\n",
    "    def categorical_to_int( self, df, labels, target='intent' ):\n",
    "        '''\n",
    "        convert the categorical intent classes to int\n",
    "        '''\n",
    "        y = df.apply( lambda row: labels.get(row[target]), axis=1 )\n",
    "        return y\n",
    "\n",
    "feature_extract = FeatureExtraction()\n",
    "# fit tfidf vectorizer\n",
    "vectorizer = feature_extract.tfidf_fit( train )\n",
    "\n",
    "# extract features on train and validation set\n",
    "train_X = feature_extract.tfidf_extract( train, vectorizer )\n",
    "train_y = feature_extract.categorical_to_int( train, labels )\n",
    "val_X = feature_extract.tfidf_extract( val, vectorizer )\n",
    "val_y = feature_extract.categorical_to_int( val, labels )\n",
    "\n",
    "train_X.shape, train_y.shape"
   ]
  },
  {
   "source": [
    "### Training on train set and evaluating the model on validation set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        20\n           1       1.00      0.80      0.89        20\n           2       1.00      1.00      1.00        20\n           3       0.92      0.60      0.73        20\n           4       0.95      0.95      0.95        20\n           5       0.94      0.75      0.83        20\n           6       1.00      0.95      0.97        20\n           7       0.86      0.95      0.90        20\n           8       0.62      1.00      0.77        20\n           9       1.00      0.70      0.82        20\n          10       1.00      0.95      0.97        20\n          11       0.83      1.00      0.91        20\n          12       1.00      1.00      1.00        20\n          13       0.95      1.00      0.98        20\n          14       1.00      0.90      0.95        20\n          15       1.00      1.00      1.00        20\n          16       1.00      1.00      1.00        20\n          17       0.94      0.85      0.89        20\n          18       0.71      1.00      0.83        20\n          19       1.00      1.00      1.00        20\n\n    accuracy                           0.92       400\n   macro avg       0.94      0.92      0.92       400\nweighted avg       0.94      0.92      0.92       400\n\n"
     ]
    }
   ],
   "source": [
    "class TextModel:\n",
    "    def __init__( self ):\n",
    "        pass\n",
    "\n",
    "    def train( self, X_train, y_train, params, n_jobs=-1, cv=3, random_state=0 ):\n",
    "        rf = RandomForestClassifier( n_jobs=-1, random_state=0 )\n",
    "        grid = GridSearchCV( rf, params, cv=3, n_jobs=-1 )\n",
    "        grid.fit( X_train, y_train )\n",
    "        return grid\n",
    "\n",
    "    def save_model( self, pickle_file, grid, labels, vectorizer, compress=3 ):\n",
    "        # save the model\n",
    "        obj = {\n",
    "            'model': grid.best_estimator_,\n",
    "            'vectorizer': vectorizer,\n",
    "            'labels': labels\n",
    "        }\n",
    "        joblib.dump( obj, pickle_file, compress=3 )\n",
    "        return\n",
    "\n",
    "    def load_model( self, pickle_file ):\n",
    "        obj = joblib.load( pickle_file )\n",
    "        return obj\n",
    "\n",
    "    def inference( self, model, X_test, y_test ):\n",
    "        y_pred = model.predict( X_test )\n",
    "        # evaluate the model\n",
    "        report = classification_report( y_test, y_pred )\n",
    "        return report\n",
    "\n",
    "pickle_file = 'my_model.pkl'\n",
    "text_model = TextModel()\n",
    "'''\n",
    "The parameters for RandomForest can be adjusted and grid search can be used\n",
    "to find the best one. RandomForest have different kind of parameter that can be\n",
    "tuned. Here, we tune for number of trees and max features\n",
    "'''\n",
    "params = { 'n_estimators': [100, 300],\n",
    "            'max_features': ['sqrt', 0.5, 1.0]\n",
    "         }\n",
    "# train RF model on the training set\n",
    "grid = text_model.train( train_X, train_y, params )\n",
    "# save the model\n",
    "text_model.save_model( pickle_file, grid, labels, vectorizer )\n",
    "# load the mmodel\n",
    "obj = text_model.load_model( pickle_file )\n",
    "# evaluate the model on the validation set\n",
    "report = text_model.inference( obj['model'], val_X, val_y )\n",
    "print( report )"
   ]
  },
  {
   "source": [
    "### Reporting results on the test set from the trained model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        30\n           1       0.96      0.77      0.85        30\n           2       1.00      0.97      0.98        30\n           3       1.00      0.87      0.93        30\n           4       0.97      0.97      0.97        30\n           5       0.91      0.97      0.94        30\n           6       1.00      1.00      1.00        30\n           7       0.90      0.93      0.92        30\n           8       0.68      1.00      0.81        30\n           9       1.00      0.90      0.95        30\n          10       0.97      0.97      0.97        30\n          11       0.97      1.00      0.98        30\n          12       1.00      0.97      0.98        30\n          13       1.00      1.00      1.00        30\n          14       1.00      0.97      0.98        30\n          15       1.00      0.97      0.98        30\n          16       1.00      0.97      0.98        30\n          17       0.92      0.77      0.84        30\n          18       0.83      1.00      0.91        30\n          19       1.00      0.97      0.98        30\n\n    accuracy                           0.95       600\n   macro avg       0.96      0.95      0.95       600\nweighted avg       0.96      0.95      0.95       600\n\n"
     ]
    }
   ],
   "source": [
    "# obtain the test set, preprocess it and extract the features\n",
    "# load the model and evaluate on the test set\n",
    "pickle_file = 'my_model.pkl'\n",
    "text_model = TextModel()\n",
    "obj = text_model.load_model( pickle_file )\n",
    "\n",
    "file = 'data_full.json'\n",
    "data = TextData( file )\n",
    "test = data.get_set( 'test', obj['labels'] )\n",
    "\n",
    "data_cleaning = TextCleaning()\n",
    "test = data_cleaning.preprocess( test )\n",
    "\n",
    "feature_extract = FeatureExtraction()\n",
    "test_X = feature_extract.tfidf_extract( test, obj['vectorizer'] )\n",
    "test_y = feature_extract.categorical_to_int( test, labels )\n",
    "# evaluate the model on the test set\n",
    "report = text_model.inference( obj['model'], test_X, test_y )\n",
    "print( report )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}