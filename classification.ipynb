{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('loyal': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ba8018fc530ee8129fef2ec252aae2b62563187ee120605478eab305cf687709"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Notebook to perform intent classification on the data from  https://github.com/clinc/oos-eval\n",
    "### Use data_full.json file "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/nikunjkotecha/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "nltk.download( 'stopwords' )"
   ]
  },
  {
   "source": [
    "### Load the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                sent        intent\n",
       "0  does ireland have any travel alerts i should b...  travel_alert\n",
       "1  does north korea have any travel alerts i shou...  travel_alert\n",
       "2             are there any travel alerts for russia  travel_alert\n",
       "3  does spain have any travel alerts i should be ...  travel_alert\n",
       "4        are there any travel alerts for north korea  travel_alert"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sent</th>\n      <th>intent</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>does ireland have any travel alerts i should b...</td>\n      <td>travel_alert</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>does north korea have any travel alerts i shou...</td>\n      <td>travel_alert</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>are there any travel alerts for russia</td>\n      <td>travel_alert</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>does spain have any travel alerts i should be ...</td>\n      <td>travel_alert</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>are there any travel alerts for north korea</td>\n      <td>travel_alert</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "class TextData:\n",
    "    def __init__( self, file ):\n",
    "        self.file = file\n",
    "    \n",
    "    def read_json( self, set='train', cols=['sent'], target='intent' ):\n",
    "        '''\n",
    "        read the json file and obtained the in-scope set\n",
    "        '''\n",
    "        with open( self.file, 'r' ) as f:\n",
    "            data = json.load( f )\n",
    "        df = pd.DataFrame( data[set], columns=cols+[target] )\n",
    "        return df\n",
    "    \n",
    "    def random_labels( self, target='intent', n=20, seed=0 ):\n",
    "        '''\n",
    "        choose 'n' random intent classes\n",
    "        '''\n",
    "        df = self.read_json( target=target )\n",
    "        np.random.seed( seed )\n",
    "        labels = np.random.choice( df[target].unique(), size=n, replace=False )\n",
    "        labels = { val:idx for idx,val in enumerate( labels ) }\n",
    "        return labels\n",
    "    \n",
    "    def get_set( self, set, labels, target='intent' ):\n",
    "        '''\n",
    "        obtained the in-scope set of given intent classes\n",
    "        '''\n",
    "        df = self.read_json( set=set )\n",
    "        df = df[ df[target].isin( labels.keys() ) ].reset_index( drop=True )\n",
    "        return df\n",
    "    \n",
    "file = 'data_full.json'\n",
    "data = TextData( file )\n",
    "# obtain random intent classes\n",
    "labels = data.random_labels()\n",
    "\n",
    "# get the training and validation set\n",
    "train = data.get_set( 'train', labels )\n",
    "val = data.get_set( 'val', labels )\n",
    "train.head()"
   ]
  },
  {
   "source": [
    "### Data Cleaning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                sent        intent  \\\n",
       "0  does ireland have any travel alerts i should b...  travel_alert   \n",
       "1  does north korea have any travel alerts i shou...  travel_alert   \n",
       "2             are there any travel alerts for russia  travel_alert   \n",
       "3  does spain have any travel alerts i should be ...  travel_alert   \n",
       "4        are there any travel alerts for north korea  travel_alert   \n",
       "\n",
       "                            clean  \n",
       "0      ireland travel alert aware  \n",
       "1  north korea travel alert aware  \n",
       "2             travel alert russia  \n",
       "3        spain travel alert aware  \n",
       "4        travel alert north korea  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sent</th>\n      <th>intent</th>\n      <th>clean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>does ireland have any travel alerts i should b...</td>\n      <td>travel_alert</td>\n      <td>ireland travel alert aware</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>does north korea have any travel alerts i shou...</td>\n      <td>travel_alert</td>\n      <td>north korea travel alert aware</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>are there any travel alerts for russia</td>\n      <td>travel_alert</td>\n      <td>travel alert russia</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>does spain have any travel alerts i should be ...</td>\n      <td>travel_alert</td>\n      <td>spain travel alert aware</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>are there any travel alerts for north korea</td>\n      <td>travel_alert</td>\n      <td>travel alert north korea</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "class TextCleaning:\n",
    "    def __init__( self ):\n",
    "        # punctuations, stop word, lemmatizer\n",
    "        self.punctuations = string.punctuation\n",
    "        self.stop_words = stopwords.words( 'english' )\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def process( self, sentence ):\n",
    "        '''\n",
    "        remove punctuations, stop word to a given sentence\n",
    "        then lemmatize each word\n",
    "        '''\n",
    "        temp = []\n",
    "        for w in word_tokenize( sentence ):\n",
    "            if w not in self.punctuations and w not in self.stop_words:\n",
    "                w = self.lemmatizer.lemmatize(w)\n",
    "                if w not in temp:\n",
    "                    temp.append(w)\n",
    "        return ' '.join( temp )\n",
    "\n",
    "    def preprocess( self, df, col='sent' ):\n",
    "        '''\n",
    "        accept a dataframe and preprocess its sentences\n",
    "        '''\n",
    "        # clean the data\n",
    "        df['clean'] = df.apply( lambda row: self.process( row[col] ), axis=1 )\n",
    "        return df\n",
    "\n",
    "data_cleaning = TextCleaning()\n",
    "# clean the training and validation set\n",
    "train = data_cleaning.preprocess( train )\n",
    "val = data_cleaning.preprocess( val )\n",
    "train.head()"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}